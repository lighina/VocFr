#!/usr/bin/env python3
"""
VocFr è¯æ±‡æ•°æ®å¯¼å…¥å·¥å…·

åŠŸèƒ½ï¼š
- ä» CSV æ–‡ä»¶è¯»å– Unite/Section/Words æ•°æ®
- è‡ªåŠ¨ç”Ÿæˆæ ‡å‡† JSON æ ¼å¼
- æ”¯æŒæ•°æ®éªŒè¯å’Œé”™è¯¯æ£€æŸ¥
- æ”¯æŒå¢é‡æ›´æ–°ï¼ˆä¸ä¼šè¦†ç›–ç°æœ‰æ•°æ®ï¼‰

ç”¨æ³•ï¼š
    python import_vocabulary.py --source vocabulary_unite4.csv --output Unite4.json
    python import_vocabulary.py --source vocabulary_unite4.csv --update --unite 4
"""

import csv
import json
import argparse
from pathlib import Path
from typing import List, Dict, Optional


class VocabularyImporter:
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.json_dir = self.project_root / "VocFr/Data/JSON"

        # è¯æ€§æ˜ å°„
        self.part_of_speech_map = {
            'noun': 'noun',
            'verb': 'verb',
            'adj': 'adjective',
            'adjective': 'adjective',
            'adv': 'adverb',
            'adverb': 'adverb',
            'prep': 'preposition',
            'preposition': 'preposition'
        }

        # æ€§åˆ«æ˜ å°„
        self.gender_map = {
            'm': 'masculine',
            'masculine': 'masculine',
            'f': 'feminine',
            'feminine': 'feminine'
        }

    def parse_csv(self, csv_path: str) -> Dict:
        """
        è§£æ CSV æ–‡ä»¶ï¼Œæå– Unite æ•°æ®

        Returns:
            {
                "unite": {...},
                "sections": [...]
            }
        """
        csv_file = Path(csv_path)
        if not csv_file.exists():
            raise FileNotFoundError(f"CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")

        unite_data = None
        sections = []
        current_section = None

        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)

            for row in reader:
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if not row or not row[0] or row[0].strip().startswith('#'):
                    continue

                # Unite ä¿¡æ¯è¡Œ
                if row[0].strip() == 'UNITE':
                    if len(row) < 7:
                        print(f"âš ï¸  Unite è¡Œæ ¼å¼ä¸æ­£ç¡®: {row}")
                        continue

                    unite_data = {
                        'id': row[1].strip(),
                        'number': int(row[2].strip()),
                        'title': row[3].strip(),
                        'titleInChinese': row[4].strip(),
                        'isUnlocked': False,
                        'requiredStars': int(row[5].strip()),
                        'requiredGems': int(row[6].strip()),
                        'sections': []
                    }
                    print(f"ğŸ“š Unite {unite_data['number']}: {unite_data['title']}")

                # Section ä¿¡æ¯è¡Œ
                elif row[0].strip() == 'SECTION':
                    if len(row) < 4:
                        print(f"âš ï¸  Section è¡Œæ ¼å¼ä¸æ­£ç¡®: {row}")
                        continue

                    # ä¿å­˜ä¹‹å‰çš„ section
                    if current_section:
                        sections.append(current_section)

                    current_section = {
                        'id': row[1].strip(),
                        'name': row[2].strip(),
                        'orderIndex': int(row[3].strip()),
                        'words': []
                    }
                    print(f"  ğŸ“‚ Section {current_section['orderIndex']}: {current_section['name']}")

                # è¯æ±‡æ•°æ®è¡Œ
                elif current_section is not None:
                    if len(row) < 6:
                        print(f"âš ï¸  è¯æ±‡è¡Œæ ¼å¼ä¸æ­£ç¡®: {row}")
                        continue

                    canonical = row[0].strip()
                    if not canonical:
                        continue

                    word = {
                        'canonical': canonical,
                        'chinese': row[1].strip(),
                        'partOfSpeech': self._normalize_part_of_speech(row[2].strip()),
                        'genderOrPos': self._normalize_gender(row[3].strip()),
                        'category': row[4].strip(),
                        'elision': row[5].strip().lower() == 'true'
                    }

                    current_section['words'].append(word)
                    print(f"    âœ“ {word['canonical']} ({word['chinese']})")

        # ä¿å­˜æœ€åä¸€ä¸ª section
        if current_section:
            sections.append(current_section)

        if not unite_data:
            raise ValueError("CSV æ–‡ä»¶ä¸­æœªæ‰¾åˆ° UNITE ä¿¡æ¯è¡Œ")

        unite_data['sections'] = sections

        return unite_data

    def _normalize_part_of_speech(self, pos: str) -> str:
        """æ ‡å‡†åŒ–è¯æ€§"""
        pos_lower = pos.lower().strip()
        return self.part_of_speech_map.get(pos_lower, pos)

    def _normalize_gender(self, gender: str) -> str:
        """æ ‡å‡†åŒ–æ€§åˆ«"""
        gender_lower = gender.lower().strip()
        return self.gender_map.get(gender_lower, gender)

    def validate_data(self, unite_data: Dict) -> List[str]:
        """
        éªŒè¯æ•°æ®å®Œæ•´æ€§

        Returns:
            é”™è¯¯åˆ—è¡¨ï¼ˆå¦‚æœä¸ºç©ºåˆ™æ•°æ®æœ‰æ•ˆï¼‰
        """
        errors = []

        # éªŒè¯ Unite
        if 'id' not in unite_data or not unite_data['id']:
            errors.append("Unite ç¼ºå°‘ id å­—æ®µ")
        if 'number' not in unite_data:
            errors.append("Unite ç¼ºå°‘ number å­—æ®µ")
        if 'title' not in unite_data or not unite_data['title']:
            errors.append("Unite ç¼ºå°‘ title å­—æ®µ")

        # éªŒè¯ Sections
        if 'sections' not in unite_data or not unite_data['sections']:
            errors.append("Unite æ²¡æœ‰ Section")
        else:
            for i, section in enumerate(unite_data['sections']):
                section_prefix = f"Section {i+1}"

                if 'id' not in section or not section['id']:
                    errors.append(f"{section_prefix} ç¼ºå°‘ id å­—æ®µ")
                if 'name' not in section or not section['name']:
                    errors.append(f"{section_prefix} ç¼ºå°‘ name å­—æ®µ")

                # éªŒè¯ Words
                if 'words' not in section or not section['words']:
                    errors.append(f"{section_prefix} æ²¡æœ‰å•è¯")
                else:
                    for j, word in enumerate(section['words']):
                        word_prefix = f"{section_prefix} Word {j+1}"

                        required_fields = ['canonical', 'chinese', 'partOfSpeech', 'genderOrPos']
                        for field in required_fields:
                            if field not in word or not word[field]:
                                errors.append(f"{word_prefix} ç¼ºå°‘ {field} å­—æ®µ")

        return errors

    def save_json(self, unite_data: Dict, output_path: str, dry_run: bool = False):
        """ä¿å­˜ä¸º JSON æ–‡ä»¶"""
        if dry_run:
            print(f"\nğŸ” é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…å†™å…¥æ–‡ä»¶")
            print(f"   å°†ä¿å­˜åˆ°: {output_path}")
            return

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(unite_data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… JSON æ–‡ä»¶å·²ä¿å­˜: {output_path}")
        print(f"   Unite ID: {unite_data['id']}")
        print(f"   Section æ•°é‡: {len(unite_data['sections'])}")
        total_words = sum(len(s['words']) for s in unite_data['sections'])
        print(f"   æ€»è¯æ±‡æ•°: {total_words}")

    def update_existing(self, unite_data: Dict, unite_number: int):
        """
        æ›´æ–°ç°æœ‰ Unite æ–‡ä»¶ï¼ˆå¢é‡æ›´æ–°ï¼‰
        """
        json_file = self.json_dir / f"Unite{unite_number}.json"

        if not json_file.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {json_file}")
            self.save_json(unite_data, str(json_file))
            return

        # è¯»å–ç°æœ‰æ•°æ®
        with open(json_file, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)

        print(f"ğŸ“– è¯»å–ç°æœ‰æ•°æ®: {json_file}")
        print(f"   ç°æœ‰ Section æ•°: {len(existing_data.get('sections', []))}")

        # åˆå¹¶ sections
        existing_section_ids = {s['id']: i for i, s in enumerate(existing_data.get('sections', []))}

        for new_section in unite_data['sections']:
            if new_section['id'] in existing_section_ids:
                # æ›´æ–°ç°æœ‰ section
                idx = existing_section_ids[new_section['id']]
                existing_data['sections'][idx] = new_section
                print(f"  ğŸ”„ æ›´æ–° Section: {new_section['name']}")
            else:
                # æ·»åŠ æ–° section
                existing_data['sections'].append(new_section)
                print(f"  â• æ·»åŠ  Section: {new_section['name']}")

        # ä¿å­˜
        self.save_json(existing_data, str(json_file))


def main():
    parser = argparse.ArgumentParser(
        description='VocFr è¯æ±‡æ•°æ®å¯¼å…¥å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

1. å¯¼å…¥æ–° Unite æ•°æ®:
   python import_vocabulary.py \\
       --source vocabulary_unite4.csv \\
       --output VocFr/Data/JSON/Unite4.json

2. æ›´æ–°ç°æœ‰ Uniteï¼ˆå¢é‡æ·»åŠ  Sectionï¼‰:
   python import_vocabulary.py \\
       --source new_sections.csv \\
       --update \\
       --unite 4

3. é¢„è§ˆå¯¼å…¥ï¼ˆä¸å®é™…å†™å…¥ï¼‰:
   python import_vocabulary.py \\
       --source vocabulary_unite4.csv \\
       --output Unite4.json \\
       --dry-run
        """
    )

    parser.add_argument('--source', '-s', required=True,
                        help='CSV æºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o',
                        help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ä½¿ç”¨ --updateï¼‰')
    parser.add_argument('--update', action='store_true',
                        help='æ›´æ–°æ¨¡å¼ï¼šå¢é‡æ·»åŠ åˆ°ç°æœ‰ Unite æ–‡ä»¶')
    parser.add_argument('--unite', '-u', type=int,
                        help='Unite ç¼–å·ï¼ˆç”¨äºæ›´æ–°æ¨¡å¼ï¼‰')
    parser.add_argument('--project', '-p', default='.',
                        help='é¡¹ç›®æ ¹ç›®å½•ï¼ˆé»˜è®¤: å½“å‰ç›®å½•ï¼‰')
    parser.add_argument('--dry-run', action='store_true',
                        help='é¢„è§ˆæ¨¡å¼ï¼šä¸å®é™…å†™å…¥æ–‡ä»¶')
    parser.add_argument('--validate-only', action='store_true',
                        help='ä»…éªŒè¯æ•°æ®ï¼Œä¸ä¿å­˜')

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if not args.update and not args.output:
        parser.error("å¿…é¡»æŒ‡å®š --output æˆ–ä½¿ç”¨ --update æ¨¡å¼")

    if args.update and not args.unite:
        parser.error("æ›´æ–°æ¨¡å¼éœ€è¦æŒ‡å®š --unite å‚æ•°")

    print("=" * 60)
    print("ğŸ“š VocFr è¯æ±‡æ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 60)
    print(f"æºæ–‡ä»¶: {args.source}")
    if args.update:
        print(f"æ¨¡å¼: æ›´æ–° Unite {args.unite}")
    else:
        print(f"è¾“å‡º: {args.output}")
    print("=" * 60)
    print()

    try:
        importer = VocabularyImporter(args.project)

        # è§£æ CSV
        print("ğŸ“– è§£æ CSV æ–‡ä»¶...")
        unite_data = importer.parse_csv(args.source)
        print()

        # éªŒè¯æ•°æ®
        print("ğŸ” éªŒè¯æ•°æ®...")
        errors = importer.validate_data(unite_data)
        if errors:
            print("âŒ æ•°æ®éªŒè¯å¤±è´¥:")
            for error in errors:
                print(f"  - {error}")
            return 1
        print("âœ… æ•°æ®éªŒè¯é€šè¿‡")
        print()

        if args.validate_only:
            print("âœ… éªŒè¯å®Œæˆï¼ˆä»…éªŒè¯æ¨¡å¼ï¼‰")
            return 0

        # ä¿å­˜æˆ–æ›´æ–°
        if args.update:
            importer.update_existing(unite_data, args.unite)
        else:
            importer.save_json(unite_data, args.output, args.dry_run)

        print()
        print("=" * 60)
        print("âœ… å¯¼å…¥å®Œæˆï¼")
        print("=" * 60)
        print()
        print("ğŸ“ ä¸‹ä¸€æ­¥:")
        print("  1. åœ¨ Xcode ä¸­å°† JSON æ–‡ä»¶æ·»åŠ åˆ°é¡¹ç›®")
        print("  2. å‡†å¤‡å¯¹åº”çš„å›¾ç‰‡èµ„æºï¼ˆå¦‚éœ€è¦ï¼‰")
        print("  3. å‡†å¤‡å¯¹åº”çš„éŸ³é¢‘èµ„æº")
        print("  4. è¿è¡Œåº”ç”¨æµ‹è¯•æ–°æ•°æ®")

        return 0

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
